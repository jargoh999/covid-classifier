# -*- coding: utf-8 -*-
"""deep learning website code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rxYLPe6nCtg34SbgvZr3IceRvFT97N_F
"""

#The dataset used in this coursework 2 was downloaded from https://www.kaggle.com/api/v1/datasets/download/sachinkumar413/covid-pneumonia-normal-chest-xray-images?dataset_version_number=1
#Downloading the dataset
#Downloads datasets from Kaggle using Kaggle API.

import kagglehub

# Download latest version
path = kagglehub.dataset_download("sachinkumar413/covid-pneumonia-normal-chest-xray-images")

print("Path to dataset files:", path)

#import os to enables interaction with the operating system.
import os

# Path to the dataset
dataset_path = os.path.expanduser("~/.cache/kagglehub/datasets/sachinkumar413/covid-pneumonia-normal-chest-xray-images/versions/1")

# List the files in the dataset directory
#dataset contains 3 subfolders
os.listdir(dataset_path)

#defining a convulational neural network model for image classification.

#import an Open-source library, tensorflow for numerical computation and large-scale machine learning
import tensorflow as tf

def create_model():


     # --- Model Architecture ---
#creating a sequential model using keras
    model = tf.keras.models.Sequential([
        # Convolutional Layers
        #for the first layer; adding a 2D convulational layer with 32 filters, 3x3 kernel, relu activation and an input shape of 224x224x3 for image input

        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
        #adding a max pooling layer with a 2x2 pool size to reduce spatial dimnensions which are the width and height
        tf.keras.layers.MaxPooling2D((2, 2)),
        #adding another 2D convulational layer with 64 filters and 3x3 kernel and Relu activation
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        #add another max pooling layer with a 2x2 pool size, further reducing the spatial dimensions
        tf.keras.layers.MaxPooling2D((2, 2)),
        #add another 2D convulational layer with 128 filters and 3x3 kernel and Relu activation
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        #add another max pooling layer with 2x2 pool size.
        tf.keras.layers.MaxPooling2D((2, 2)),

        #moving from feature extraction to classification for making predictions
        #Flatten and Dense Layers
        #flattening the output from the convolutional layers into a 1D vector
        tf.keras.layers.Flatten(),
        #adding a dense layer with 128 neurons and Relu activation
        tf.keras.layers.Dense(128, activation='relu'),
        #adding a final dense layer with 3 neurons and softmax activation.
        tf.keras.layers.Dense(3, activation='softmax')  # 3 output classes: COVID-19, Pneumonia, Normal
    ])
    #configuring the learning process of the model
    # --- Hyperparameters and Compilation ---
    model.compile(optimizer='adam', #Adam optimizer adapts the learning rate based on how well its doing leading to faster learning
                  loss='categorical_crossentropy',  #Use sparse_categorical_crossentropy if labels are integers
  #this metrics evaluate the model's performance to monitor how well the model is doing.
                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])

    return model

# Create an instance of the model
model = create_model()

# Print model summary
model.summary()

#import an Open-source library, tensorflow for numerical computation and large-scale machine learning
import tensorflow as tf
#Imports the ImageDataGenerator class from TensorFlow's Keras preprocessing module.
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#Define paths
train_data_dir = os.path.expanduser("~/.cache/kagglehub/datasets/sachinkumar413/covid-pneumonia-normal-chest-xray-images/versions/1")

# Create an ImageDataGenerator for data augmentation and preprocessing
train_datagen = ImageDataGenerator(
    rescale=1./255,  #Normalize pixel values to [0, 1] to improve model performance
    shear_range=0.2, #introducing variations to make the model perform well
    zoom_range=0.2, #enlarge or shrinking some portion of the image
    horizontal_flip=True, #flip images horizontally for data augumentation
    validation_split=0.2  #Split data into training and validation sets
)

# Create data generators for training and validation sets
# Create a training data generator using the ImageDataGenerator and the specified directory.
train_generator = train_datagen.flow_from_directory(
    train_data_dir, #Specifies the directory where the training data is located.
    target_size=(224, 224), #resize all images to ensure same dimension to input into model`
    batch_size=32, #A batch size of 32 means that the model will be trained on 32 images at a time before updating its weights.
    class_mode='categorical', #'categorical' indicates that the labels will be one-hot encoded vectors, representing the different classes in the dataset.
    subset='training' #Specifies that this generator is for the training subset of the data.
)
#Creates a validation data generator using the same ImageDataGenerator for the validation subset of the data
validation_generator = train_datagen.flow_from_directory(
    train_data_dir, #Specifies the directory where the validation data is located.
    target_size=(224, 224), #resize all images to ensure same dimension to input into model
    batch_size=32, #A batch size of 32 means that the model will be trained on 32 images at a time before updating its weights.
    class_mode='categorical', #categorical', #'categorical' indicates that the labels will be one-hot encoded vectors, representing the different classes in the dataset
    subset='validation' #Specifies that this generator is for the validation subset of the data.
)

# Train the model
history = model.fit(
    train_generator, #This generator provides batches of images and labels for the model to learn from during each training epoch
    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Calculate steps per epoch
    epochs=10,  # Set the number of epochs
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size  # Calculate validation steps
)

# Save the trained model as covid_classifier.h5
model.save("covid_classifier.h5")